{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from itertools import combinations_with_replacement\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# -- Perturbation registry --\n",
    "from textattack.transformations import (\n",
    "    WordSwapHomoglyphSwap,\n",
    "    WordSwapDeletions,\n",
    "    WordSwapInvisibleCharacters,\n",
    "    WordSwapReorderings,\n",
    "    WordSwapNeighboringCharacterSwap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from itertools import combinations_with_replacement\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "import textattack\n",
    "from textattack.transformations import (\n",
    "    WordSwapHomoglyphSwap,\n",
    "    WordSwapInvisibleCharacters,\n",
    "    WordSwapDeletions,\n",
    "    WordSwapReorderings,\n",
    "    WordSwapNeighboringCharacterSwap\n",
    ")\n",
    "\n",
    "# Perturbation definitions\n",
    "PERTURBATIONS = {\n",
    "    1: WordSwapHomoglyphSwap,\n",
    "    2: WordSwapInvisibleCharacters,\n",
    "    3: WordSwapDeletions,\n",
    "    4: WordSwapReorderings,\n",
    "    5: WordSwapNeighboringCharacterSwap\n",
    "}\n",
    "\n",
    "def load_base_texts(dataset_name: str = \"emotion\", split: str = \"train\", min_words: int = 5, max_samples: int = 20000) -> List[Tuple[str, int]]:\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    texts = []\n",
    "    for i, example in enumerate(dataset):\n",
    "        text = example[\"text\"].strip()\n",
    "        label = example[\"label\"]\n",
    "        if len(text.split()) >= min_words:\n",
    "            texts.append((text, label))\n",
    "        if len(texts) >= max_samples:\n",
    "            break\n",
    "    print(\"Sample:\", texts[0])\n",
    "    return texts\n",
    "\n",
    "def apply_perturbations(text: str, perturb_seq: Tuple[int], same_word: bool = False) -> str:\n",
    "    attacked_text = textattack.shared.AttackedText(text)\n",
    "    num_words = len(attacked_text.words)\n",
    "    if num_words == 0:\n",
    "        return text\n",
    "\n",
    "    if same_word:\n",
    "        idx = random.randint(0, num_words - 1)\n",
    "        for p in perturb_seq:\n",
    "            tform = PERTURBATIONS[p](random_one=True)\n",
    "            transformed_texts = tform._get_transformations(attacked_text, [idx])\n",
    "            if transformed_texts:\n",
    "                attacked_text = transformed_texts[0]\n",
    "    else:\n",
    "        for p in perturb_seq:\n",
    "            tform = PERTURBATIONS[p](random_one=True)\n",
    "            idx = random.randint(0, num_words - 1)\n",
    "            transformed_texts = tform._get_transformations(attacked_text, [idx])\n",
    "            if transformed_texts:\n",
    "                attacked_text = transformed_texts[0]\n",
    "\n",
    "    return attacked_text.text\n",
    "\n",
    "def generate_dataset_weighted_with_labels(base_texts: List[Tuple[str, int]],\n",
    "                                          perturbation_allocation: Dict[Tuple[int], int],\n",
    "                                          path: str,\n",
    "                                          same_word: bool = False):\n",
    "    rows = []\n",
    "    for seq, n_samples in perturbation_allocation.items():\n",
    "        sampled = random.sample(base_texts, min(n_samples, len(base_texts)))\n",
    "        for clean, label in sampled:\n",
    "            perturbed = apply_perturbations(clean, seq, same_word=same_word)\n",
    "            rows.append((perturbed, clean, label, str(seq)))\n",
    "\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"input\", \"original_text\", \"label\", \"perturbation\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Saved {len(rows)} examples → {path}\")\n",
    "\n",
    "def allocate_dataset_examples(perturbation_groups: Dict[str, List[Tuple[int]]],\n",
    "                              proportions: Dict[str, float],\n",
    "                              total_samples: int) -> Dict[Tuple[int], int]:\n",
    "    allocation = {}\n",
    "    for group_name, perturbations in perturbation_groups.items():\n",
    "        group_total = int(proportions[group_name] * total_samples)\n",
    "        per_combo = group_total // len(perturbations)\n",
    "        for seq in perturbations:\n",
    "            allocation[seq] = per_combo\n",
    "    return allocation\n",
    "\n",
    "def run_experiment_weighted_with_labels(config_name: str,\n",
    "                                        train_groups: Dict[str, List[Tuple[int]]],\n",
    "                                        train_props: Dict[str, float],\n",
    "                                        ft_groups: Dict[str, List[Tuple[int]]],\n",
    "                                        ft_props: Dict[str, float],\n",
    "                                        base_texts: List[Tuple[str, int]],\n",
    "                                        train_out: str,\n",
    "                                        ft_out: str,\n",
    "                                        total_train: int,\n",
    "                                        total_ft: int,\n",
    "                                        same_word_train: bool = False,\n",
    "                                        same_word_ft: bool = False):\n",
    "    print(f\"[TRAIN] {config_name}\")\n",
    "    train_allocation = allocate_dataset_examples(train_groups, train_props, total_train)\n",
    "    generate_dataset_weighted_with_labels(base_texts, train_allocation, train_out, same_word=same_word_train)\n",
    "\n",
    "    print(f\"[FINETUNE] {config_name}\")\n",
    "    ft_allocation = allocate_dataset_examples(ft_groups, ft_props, total_ft)\n",
    "    generate_dataset_weighted_with_labels(base_texts, ft_allocation, ft_out, same_word=same_word_ft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: ('i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake', 0)\n",
      "[TRAIN] exp1\n",
      "Saved 4000 examples → data/exp1_train.csv\n",
      "[FINETUNE] exp1\n",
      "Saved 1000 examples → data/exp1_ft.csv\n",
      "[TRAIN] exp2\n",
      "Saved 4000 examples → data/exp2_train.csv\n",
      "[FINETUNE] exp2\n",
      "Saved 1000 examples → data/exp2_ft.csv\n",
      "[TRAIN] exp3\n",
      "Saved 4000 examples → data/exp3_train.csv\n",
      "[FINETUNE] exp3\n",
      "Saved 996 examples → data/exp3_ft.csv\n",
      "[TRAIN] exp4\n",
      "Saved 4000 examples → data/exp4_train.csv\n",
      "[FINETUNE] exp4\n",
      "Saved 1000 examples → data/exp4_ft.csv\n",
      "[TRAIN] exp5\n",
      "Saved 4000 examples → data/exp5_train.csv\n",
      "[FINETUNE] exp5\n",
      "Saved 1000 examples → data/exp5_ft.csv\n",
      "[TRAIN] exp6_same_word_2mix\n",
      "Saved 4000 examples → data/exp6_train.csv\n",
      "[FINETUNE] exp6_same_word_2mix\n",
      "Saved 1000 examples → data/exp6_ft.csv\n",
      "[TRAIN] exp7_same_word_full_mix\n",
      "Saved 4000 examples → data/exp7_train.csv\n",
      "[FINETUNE] exp7_same_word_full_mix\n",
      "Saved 1000 examples → data/exp7_ft.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run 7 experiments\n",
    "if __name__ == \"__main__\":\n",
    "    base_texts = load_base_texts(\"dair-ai/emotion\", \"train\", min_words=5, max_samples=20000)\n",
    "    p1234 = [1, 2, 3, 4]\n",
    "    one_mix = [(i,) for i in p1234]\n",
    "    two_mix = list(combinations_with_replacement(p1234, 2))\n",
    "    three_mix = list(combinations_with_replacement(p1234, 3))\n",
    "    same_word_two_mix = two_mix\n",
    "    same_word_three_mix = three_mix\n",
    "\n",
    "    # Exp 1\n",
    "    run_experiment_weighted_with_labels(\"exp1\",\n",
    "        {\"one_mix\": one_mix}, {\"one_mix\": 1.0},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp1_train.csv\", \"data/exp1_ft.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 2\n",
    "    run_experiment_weighted_with_labels(\"exp2\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix}, {\"one_mix\": 0.5, \"two_mix\": 0.5},\n",
    "        {\"pure_5\": [(5,)], \"mixed_5\": [(i, 5) for i in p1234]}, {\"pure_5\": 0.5, \"mixed_5\": 0.5},\n",
    "        base_texts, \"data/exp2_train.csv\", \"data/exp2_ft.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 3\n",
    "    run_experiment_weighted_with_labels(\"exp3\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix, \"three_mix\": three_mix},\n",
    "        {\"one_mix\": 0.25, \"two_mix\": 0.35, \"three_mix\": 0.4},\n",
    "        {\n",
    "            \"pure_5\": [(5,)],\n",
    "            \"i_5\": [(i, 5) for i in p1234],\n",
    "            \"ij_5\": [(i, j, 5) for i in p1234 for j in p1234]\n",
    "        },\n",
    "        {\"pure_5\": 0.2, \"i_5\": 0.3, \"ij_5\": 0.5},\n",
    "        base_texts, \"data/exp3_train.csv\", \"data/exp3_ft.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 4\n",
    "    run_experiment_weighted_with_labels(\"exp4\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix}, {\"one_mix\": 0.5, \"two_mix\": 0.5},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp4_train.csv\", \"data/exp4_ft.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 5\n",
    "    run_experiment_weighted_with_labels(\"exp5\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix, \"three_mix\": three_mix},\n",
    "        {\"one_mix\": 0.25, \"two_mix\": 0.35, \"three_mix\": 0.4},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp5_train.csv\", \"data/exp5_ft.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 6: Same-word 2-mix\n",
    "    run_experiment_weighted_with_labels(\"exp6_same_word_2mix\",\n",
    "        {\"same_word_two_mix\": same_word_two_mix}, {\"same_word_two_mix\": 1.0},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp6_train.csv\", \"data/exp6_ft.csv\", 4000, 1000,\n",
    "        same_word_train=True, same_word_ft=False)\n",
    "\n",
    "    # Exp 7: Same-word 1+2+3-mix\n",
    "    run_experiment_weighted_with_labels(\"exp7_same_word_full_mix\",\n",
    "        {\n",
    "            \"one_mix\": one_mix,\n",
    "            \"same_word_two_mix\": same_word_two_mix,\n",
    "            \"same_word_three_mix\": same_word_three_mix\n",
    "        },\n",
    "        {\"one_mix\": 0.25, \"same_word_two_mix\": 0.35, \"same_word_three_mix\": 0.4},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp7_train.csv\", \"data/exp7_ft.csv\", 4000, 1000,\n",
    "        same_word_train=True, same_word_ft=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: ('im feeling rather rotten so im not very ambitious right now', 0)\n",
      "[TRAIN] exp1\n",
      "Saved 4000 examples → data/exp1_train_test.csv\n",
      "[FINETUNE] exp1\n",
      "Saved 1000 examples → data/exp1_ft_test.csv\n",
      "[TRAIN] exp2\n",
      "Saved 4000 examples → data/exp2_train_test.csv\n",
      "[FINETUNE] exp2\n",
      "Saved 1000 examples → data/exp2_ft_test.csv\n",
      "[TRAIN] exp3\n",
      "Saved 4000 examples → data/exp3_train_test.csv\n",
      "[FINETUNE] exp3\n",
      "Saved 996 examples → data/exp3_ft_test.csv\n",
      "[TRAIN] exp4\n",
      "Saved 4000 examples → data/exp4_train_test.csv\n",
      "[FINETUNE] exp4\n",
      "Saved 1000 examples → data/exp4_ft_test.csv\n",
      "[TRAIN] exp5\n",
      "Saved 4000 examples → data/exp5_train_test.csv\n",
      "[FINETUNE] exp5\n",
      "Saved 1000 examples → data/exp5_ft_test.csv\n",
      "[TRAIN] exp6_same_word_2mix\n",
      "Saved 4000 examples → data/exp6_train_test.csv\n",
      "[FINETUNE] exp6_same_word_2mix\n",
      "Saved 1000 examples → data/exp6_ft_test.csv\n",
      "[TRAIN] exp7_same_word_full_mix\n",
      "Saved 4000 examples → data/exp7_train_test.csv\n",
      "[FINETUNE] exp7_same_word_full_mix\n",
      "Saved 1000 examples → data/exp7_ft_test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run 7 experiments\n",
    "if __name__ == \"__main__\":\n",
    "    base_texts = load_base_texts(\"dair-ai/emotion\", \"test\", min_words=5, max_samples=20000)\n",
    "    p1234 = [1, 2, 3, 4]\n",
    "    one_mix = [(i,) for i in p1234]\n",
    "    two_mix = list(combinations_with_replacement(p1234, 2))\n",
    "    three_mix = list(combinations_with_replacement(p1234, 3))\n",
    "    same_word_two_mix = two_mix\n",
    "    same_word_three_mix = three_mix\n",
    "\n",
    "    # Exp 1\n",
    "    run_experiment_weighted_with_labels(\"exp1\",\n",
    "        {\"one_mix\": one_mix}, {\"one_mix\": 1.0},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp1_train_test.csv\", \"data/exp1_ft_test.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 2\n",
    "    run_experiment_weighted_with_labels(\"exp2\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix}, {\"one_mix\": 0.5, \"two_mix\": 0.5},\n",
    "        {\"pure_5\": [(5,)], \"mixed_5\": [(i, 5) for i in p1234]}, {\"pure_5\": 0.5, \"mixed_5\": 0.5},\n",
    "        base_texts, \"data/exp2_train_test.csv\", \"data/exp2_ft_test.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 3\n",
    "    run_experiment_weighted_with_labels(\"exp3\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix, \"three_mix\": three_mix},\n",
    "        {\"one_mix\": 0.25, \"two_mix\": 0.35, \"three_mix\": 0.4},\n",
    "        {\n",
    "            \"pure_5\": [(5,)],\n",
    "            \"i_5\": [(i, 5) for i in p1234],\n",
    "            \"ij_5\": [(i, j, 5) for i in p1234 for j in p1234]\n",
    "        },\n",
    "        {\"pure_5\": 0.2, \"i_5\": 0.3, \"ij_5\": 0.5},\n",
    "        base_texts, \"data/exp3_train_test.csv\", \"data/exp3_ft_test.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 4\n",
    "    run_experiment_weighted_with_labels(\"exp4\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix}, {\"one_mix\": 0.5, \"two_mix\": 0.5},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp4_train_test.csv\", \"data/exp4_ft_test.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 5\n",
    "    run_experiment_weighted_with_labels(\"exp5\",\n",
    "        {\"one_mix\": one_mix, \"two_mix\": two_mix, \"three_mix\": three_mix},\n",
    "        {\"one_mix\": 0.25, \"two_mix\": 0.35, \"three_mix\": 0.4},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp5_train_test.csv\", \"data/exp5_ft_test.csv\", 4000, 1000)\n",
    "\n",
    "    # Exp 6: Same-word 2-mix\n",
    "    run_experiment_weighted_with_labels(\"exp6_same_word_2mix\",\n",
    "        {\"same_word_two_mix\": same_word_two_mix}, {\"same_word_two_mix\": 1.0},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp6_train_test.csv\", \"data/exp6_ft_test.csv\", 4000, 1000,\n",
    "        same_word_train=True, same_word_ft=False)\n",
    "\n",
    "    # Exp 7: Same-word 1+2+3-mix\n",
    "    run_experiment_weighted_with_labels(\"exp7_same_word_full_mix\",\n",
    "        {\n",
    "            \"one_mix\": one_mix,\n",
    "            \"same_word_two_mix\": same_word_two_mix,\n",
    "            \"same_word_three_mix\": same_word_three_mix\n",
    "        },\n",
    "        {\"one_mix\": 0.25, \"same_word_two_mix\": 0.35, \"same_word_three_mix\": 0.4},\n",
    "        {\"pure_5\": [(5,)]}, {\"pure_5\": 1.0},\n",
    "        base_texts, \"data/exp7_train_test.csv\", \"data/exp7_ft_test.csv\", 4000, 1000,\n",
    "        same_word_train=True, same_word_ft=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/125 [00:07<16:01,  7.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 211\u001b[0m\n\u001b[1;32m    208\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    210\u001b[0m     run(csv_path, use_semantic_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mrun_exp1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 210\u001b[0m, in \u001b[0;36mrun_exp1\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/exp1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    208\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(csv_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 210\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_semantic_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 184\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(csv_path, use_semantic_loss)\u001b[0m\n\u001b[1;32m    182\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[1;32m    183\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m)\n\u001b[0;32m--> 184\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_sem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_semantic_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m acc \u001b[38;5;241m=\u001b[39m evaluate(model, loader, device)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, acc)\n",
      "Cell \u001b[0;32mIn[19], line 124\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device, lambda_sem)\u001b[0m\n\u001b[1;32m    121\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens_pert\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    122\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 124\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m    127\u001b[0m emb_clean \u001b[38;5;241m=\u001b[39m get_embeddings(model, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens_clean\u001b[39m\u001b[38;5;124m'\u001b[39m], device)  \u001b[38;5;66;03m# (B, T, H)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:976\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    974\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 976\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    986\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:796\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    792\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_attention_mask_for_sdpa(\n\u001b[1;32m    793\u001b[0m             attention_mask, embeddings\u001b[38;5;241m.\u001b[39mdtype, tgt_len\u001b[38;5;241m=\u001b[39minput_shape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    794\u001b[0m         )\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    541\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    542\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    543\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m         output_attentions,\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 549\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:475\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    484\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/disspt2/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:401\u001b[0m, in \u001b[0;36mDistilBertSdpaAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    398\u001b[0m     k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    399\u001b[0m     v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 401\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m unshape(attn_output)\n\u001b[1;32m    411\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_lin(attn_output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === 📦 Dependencies ===\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import os, ast\n",
    "\n",
    "# === 🧾 Adapted from TextAttack-based perturbation ===\n",
    "from textattack.transformations import (\n",
    "    WordSwapHomoglyphSwap,\n",
    "    WordSwapInvisibleCharacters,\n",
    "    WordSwapDeletions,\n",
    "    WordSwapReorderings,\n",
    "    WordSwapNeighboringCharacterSwap\n",
    ")\n",
    "import textattack\n",
    "\n",
    "PERTURBATIONS = {\n",
    "    1: WordSwapHomoglyphSwap,\n",
    "    2: WordSwapInvisibleCharacters,\n",
    "    3: WordSwapDeletions,\n",
    "    4: WordSwapReorderings,\n",
    "    5: WordSwapNeighboringCharacterSwap\n",
    "}\n",
    "\n",
    "def perturb_and_indices(text: str, perturb_seq=(1,), same_word=False):\n",
    "    attacked_text = textattack.shared.AttackedText(text)\n",
    "    num_words = len(attacked_text.words)\n",
    "    if num_words == 0:\n",
    "        return text, text, [[i] for i in range(num_words)], [[i] for i in range(num_words)]\n",
    "\n",
    "    pert_text = attacked_text\n",
    "    if same_word:\n",
    "        idx = random.randint(0, num_words - 1)\n",
    "        for p in perturb_seq:\n",
    "            tform = PERTURBATIONS[p](random_one=True)\n",
    "            transformed = tform._get_transformations(pert_text, [idx])\n",
    "            if transformed:\n",
    "                pert_text = transformed[0]\n",
    "    else:\n",
    "        for p in perturb_seq:\n",
    "            idx = random.randint(0, num_words - 1)\n",
    "            tform = PERTURBATIONS[p](random_one=True)\n",
    "            transformed = tform._get_transformations(pert_text, [idx])\n",
    "            if transformed:\n",
    "                pert_text = transformed[0]\n",
    "\n",
    "    clean_words = attacked_text.words\n",
    "    pert_words = pert_text.words\n",
    "\n",
    "    # Naive assumption: same number of words → one-to-one index mapping\n",
    "    clean_indices = [[i] for i in range(len(clean_words))]\n",
    "    pert_indices = [[i] for i in range(len(pert_words))]\n",
    "\n",
    "    return pert_text.text, text, pert_indices, clean_indices\n",
    "\n",
    "# === 📦 Generate CSV ===\n",
    "def generate_csv(out_path, n=500):\n",
    "    dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "    rows = []\n",
    "    for ex in dataset.select(range(n)):\n",
    "        clean_text = ex['text']\n",
    "        label = ex['label']\n",
    "        perturbed, clean, p_idx, c_idx = perturb_and_indices(clean_text)\n",
    "        rows.append([perturbed, clean, label, str(p_idx), str(c_idx)])\n",
    "    df = pd.DataFrame(rows, columns=[\"input\", \"original_text\", \"label\", \"word_indices_perturbed\", \"word_indices_clean\"])\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "# === 📚 Dataset Loader ===\n",
    "class SemanticDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        df = pd.read_csv(path)\n",
    "        self.input = df[\"input\"].tolist()\n",
    "        self.clean = df[\"original_text\"].tolist()\n",
    "        self.label = df[\"label\"].tolist()\n",
    "        self.pidx = [ast.literal_eval(x) for x in df[\"word_indices_perturbed\"]]\n",
    "        self.cidx = [ast.literal_eval(x) for x in df[\"word_indices_clean\"]]\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        t_clean = self.tokenizer(self.clean[i], return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "        t_pert = self.tokenizer(self.input[i], return_tensors=\"pt\", truncation=True, padding=\"max_length\")\n",
    "        return {\n",
    "            \"tokens_clean\": {k: v.squeeze(0) for k, v in t_clean.items()},\n",
    "            \"tokens_pert\": {k: v.squeeze(0) for k, v in t_pert.items()},\n",
    "            \"cidx\": self.cidx[i], \"pidx\": self.pidx[i],\n",
    "            \"label\": torch.tensor(self.label[i])\n",
    "        }\n",
    "\n",
    "# === 🧱 Semantic Loss Utils ===\n",
    "def get_embeddings(model, tokens, device):\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items() if k in {\"input_ids\", \"attention_mask\"}}\n",
    "    with torch.no_grad():\n",
    "        return model.distilbert(**tokens).last_hidden_state  # shape: (B, T, H)\n",
    "\n",
    "\n",
    "def get_word_embeddings(hidden, index_list):\n",
    "    word_embeds = []\n",
    "    for group in index_list:\n",
    "        vec = sum(hidden[i] for i in group if i < hidden.size(0)) / len(group)\n",
    "        word_embeds.append(vec)\n",
    "    return torch.stack(word_embeds)\n",
    "\n",
    "# === 🏋️ Training Loop ===\n",
    "def train(model, loader, optimizer, device, lambda_sem):\n",
    "    model.train()\n",
    "    for batch in tqdm(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['tokens_pert']['input_ids'].to(device)\n",
    "        attention_mask = batch['tokens_pert']['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss_cls = output.loss\n",
    "\n",
    "        emb_clean = get_embeddings(model, batch['tokens_clean'], device)  # (B, T, H)\n",
    "        emb_pert = get_embeddings(model, batch['tokens_pert'], device)    # (B, T, H)\n",
    "\n",
    "        emb_clean_words_list = [\n",
    "            get_word_embeddings(emb_clean[i], batch['cidx'][i]) for i in range(len(batch['cidx']))\n",
    "        ]\n",
    "        emb_pert_words_list = [\n",
    "            get_word_embeddings(emb_pert[i], batch['pidx'][i]) for i in range(len(batch['pidx']))\n",
    "        ]\n",
    "\n",
    "        loss_sem_list = [\n",
    "            1 - F.cosine_similarity(clean_w, pert_w, dim=1).mean()\n",
    "            for clean_w, pert_w in zip(emb_clean_words_list, emb_pert_words_list)\n",
    "        ]\n",
    "\n",
    "        loss_sem = torch.stack(loss_sem_list).mean()\n",
    "        loss = loss_cls + lambda_sem * loss_sem\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# === 🧪 Evaluation ===\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            logits = model(**{k: v.to(device) for k, v in batch['tokens_pert'].items()}).logits\n",
    "            pred = torch.argmax(logits, dim=-1).cpu().tolist()\n",
    "            preds += pred\n",
    "            trues += batch['label'].tolist()\n",
    "    return accuracy_score(trues, preds)\n",
    "\n",
    "# === 🤝 Custom Collate Function ===\n",
    "def custom_collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    collated = {}\n",
    "    for key in keys:\n",
    "        if isinstance(batch[0][key], dict):\n",
    "            collated[key] = {\n",
    "                subkey: torch.stack([item[key][subkey] for item in batch])\n",
    "                for subkey in batch[0][key].keys()\n",
    "            }\n",
    "        elif isinstance(batch[0][key], list):\n",
    "            collated[key] = [item[key] for item in batch]\n",
    "        else:\n",
    "            collated[key] = torch.stack([item[key] for item in batch])\n",
    "    return collated\n",
    "\n",
    "# === 🚀 Main ===\n",
    "def run(csv_path, use_semantic_loss=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"bhadresh-savani/distilbert-base-uncased-emotion\", num_labels=6).to(device)\n",
    "    dataset = SemanticDataset(csv_path, tokenizer)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    train(model, loader, optimizer, device, lambda_sem=0.5 if use_semantic_loss else 0.0)\n",
    "    acc = evaluate(model, loader, device)\n",
    "    print(\"Accuracy:\", acc)\n",
    "\n",
    "\n",
    "def run_exp1():\n",
    "    from itertools import combinations_with_replacement\n",
    "\n",
    "    base_texts = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "    base_texts = [(ex['text'].strip(), ex['label']) for ex in base_texts if len(ex['text'].strip().split()) >= 5][:5000]\n",
    "\n",
    "    one_mix = [(i,) for i in [1, 2, 3, 4]]\n",
    "    allocation = {seq: 1000 // len(one_mix) for seq in one_mix}  # 1000 examples evenly split\n",
    "\n",
    "    rows = []\n",
    "    for seq, n_samples in allocation.items():\n",
    "        sampled = random.sample(base_texts, n_samples)\n",
    "        for clean, label in sampled:\n",
    "            perturbed, _, p_idx, c_idx = perturb_and_indices(clean, seq, same_word=False)\n",
    "            rows.append([perturbed, clean, label, str(p_idx), str(c_idx)])\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"input\", \"original_text\", \"label\", \"word_indices_perturbed\", \"word_indices_clean\"])\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    csv_path = \"data/exp1.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    run(csv_path, use_semantic_loss=True)\n",
    "run_exp1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside: <AttackedText \"hello hellо2 hello3 hello4 hello5\">\n",
      "'hellо2'\n",
      "'helm\\x08lо2'\n",
      "inside: <AttackedText \"hello hellо2 hello3 hello4 hello5\">\n",
      "inside: <AttackedText \"hello hel‭⁦‮⁩⁦m⁩‬⁩‬lо2 hello3 hello4 hello5\">\n",
      "hello hel‭⁦‮⁩⁦m⁩‬⁩‬lо2 hello3 hello4 hello5\n"
     ]
    }
   ],
   "source": [
    "print(apply_perturbations(\"hello hello2 hello3 hello4 hello5\", (1, 3, 4), same_word=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run this on colab\n",
    "Training + finetuning T5\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    ")\n",
    "import Levenshtein  \n",
    "\n",
    "def finetune_t5_on_experiment(csv_path, out_dir, model_id=\"t5-small\", val_split=0.03,\n",
    "                              batch_size=16, epochs=3, lr=5e-4, max_len=128, seed=42):\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    ds = load_dataset(\"csv\", data_files=csv_path)[\"train\"].shuffle(seed=seed)\n",
    "    split = ds.train_test_split(test_size=val_split, seed=seed)\n",
    "    train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "    def preprocess(batch):\n",
    "        model_in = tokenizer(batch[\"input\"], padding=\"longest\", truncation=True, max_length=max_len)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(batch[\"original_text\"], padding=\"longest\", truncation=True, max_length=max_len).input_ids\n",
    "        model_in[\"labels\"] = labels\n",
    "        return model_in\n",
    "\n",
    "    train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "    val_ds = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        learning_rate=lr,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        do_eval=True,\n",
    "        generation_max_length=max_len,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        label_smoothing_factor=0.1,\n",
    "        save_total_limit=1,\n",
    "        seed=seed,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    def postprocess_text(preds, labels):\n",
    "        return [p.strip() for p in preds], [l.strip() for l in labels]\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        preds, labels = postprocess_text(preds, labels)\n",
    "\n",
    "        def norm_edit_sim(p, l):\n",
    "            return 1.0 - Levenshtein.distance(p, l) / max(1, max(len(p), len(l)))\n",
    "\n",
    "        avg_sim = np.mean([norm_edit_sim(p, l) for p, l in zip(preds, labels)])\n",
    "        return {\"norm_edit_sim\": avg_sim}\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "    print(f\"Finished fine-tuning {csv_path} → saved to {out_dir}\")\n",
    "\n",
    "for i in range(1, 8):\n",
    "    finetune_t5_on_experiment(f\"exp{i}_train.csv\", f\"models/exp{i}_train\")\n",
    "    finetune_t5_on_experiment(\n",
    "        csv_path = f\"exp{i}_ft.csv\",\n",
    "        out_dir = f\"models/exp{i}_ft\",\n",
    "        model_id = f\"models/exp{i}_train\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not lose this\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from transformers.models.t5.modeling_t5 import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Stack,\n",
    "    T5Block,\n",
    "    T5Attention\n",
    ")\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Trust Gate\n",
    "# ------------------------------------------\n",
    "\n",
    "class TrustGate(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # Output shape: (batch_size, seq_len)\n",
    "        return self.gate(hidden_states).squeeze(-1)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Trust-Aware Self-Attention\n",
    "# ------------------------------------------\n",
    "\n",
    "class TrustT5Attention(T5Attention):\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__(config, has_relative_attention_bias)\n",
    "        self.trust_gate = TrustGate(config.d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        mask=None,\n",
    "        position_bias=None,\n",
    "        past_key_value=None,\n",
    "        layer_head_mask=None,\n",
    "        query_length=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "        key_value_states=None,\n",
    "        cache_position=None\n",
    "    ):\n",
    "        # Always request attention weights and full outputs\n",
    "        output = super().forward(\n",
    "            hidden_states=hidden_states,\n",
    "            mask=mask,\n",
    "            position_bias=position_bias,\n",
    "            past_key_value=past_key_value,\n",
    "            layer_head_mask=layer_head_mask,\n",
    "            query_length=query_length,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=True,  # needed for attn_weights\n",
    "            key_value_states=key_value_states,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "\n",
    "        # Unpack\n",
    "        attention_output = output[0]\n",
    "        present_key_value = output[1]\n",
    "        position_bias_out = output[2] if len(output) > 2 else None\n",
    "        attn_weights = output[3] if len(output) > 3 else None\n",
    "\n",
    "        # Apply trust gate\n",
    "        if attn_weights is not None:\n",
    "            trust = self.trust_gate(hidden_states).unsqueeze(1).unsqueeze(2)  # (B, 1, 1, L)\n",
    "            attn_weights = attn_weights * trust\n",
    "\n",
    "        # Return all expected values (even if some are None)\n",
    "        outputs = (attention_output, present_key_value, position_bias_out)\n",
    "        if output_attentions:\n",
    "            outputs += (attn_weights,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Block and Stack\n",
    "# ------------------------------------------\n",
    "\n",
    "class TrustBlock(T5Block):\n",
    "    def __init__(self, config, has_relative_attention_bias=False):\n",
    "        super().__init__(config, has_relative_attention_bias)\n",
    "        self.layer[0].SelfAttention = TrustT5Attention(config, has_relative_attention_bias)\n",
    "\n",
    "\n",
    "class TrustEncoder(T5Stack):\n",
    "    def __init__(self, config, embed_tokens):\n",
    "        config = copy.deepcopy(config)\n",
    "        config.is_decoder = False\n",
    "        config.use_cache = False\n",
    "        super().__init__(config, embed_tokens)\n",
    "\n",
    "        self.block = nn.ModuleList([\n",
    "            TrustBlock(config, has_relative_attention_bias=(i == 0))\n",
    "            for i in range(config.num_layers)\n",
    "        ])\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Model Definition\n",
    "# ------------------------------------------\n",
    "\n",
    "class TrustT5(T5ForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.encoder = TrustEncoder(config, self.shared)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name, *args, **kwargs):\n",
    "        model = super().from_pretrained(model_name, *args, **kwargs)\n",
    "        model.encoder = TrustEncoder(model.config, model.shared)\n",
    "        return model\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = TrustT5.from_pretrained(\"t5-small\")\n",
    "text = \"emotion: I am so proud of you\"\n",
    "label_text = \"joy\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "labels = tokenizer(label_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "generated_ids = model.generate(**inputs)\n",
    "decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated output:\", decoded_output)\n",
    "\n",
    "print(\"Loss:\", outputs.loss.item())\n",
    "print(\"Logits shape:\", outputs.logits.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont fkin lose this one either\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "# Check device availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create directories for saving model and logs\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Load the tokenizer and model (from pre-trained checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# Ensure padding token is set to eos token for T5\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def preprocess_data(csv_path, tokenizer, max_length=128):\n",
    "    df = pd.read_csv(csv_path)  # Load dataset\n",
    "    dataset = Dataset.from_pandas(df)  # Convert to Hugging Face dataset\n",
    "\n",
    "    # Preprocessing function to tokenize inputs and targets\n",
    "    def preprocess(batch):\n",
    "        model_in = tokenizer(batch[\"input\"], padding=\"longest\", truncation=True, max_length=max_length)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(batch[\"original_text\"], padding=\"longest\", truncation=True, max_length=max_length).input_ids\n",
    "        model_in[\"labels\"] = labels\n",
    "        return model_in\n",
    "\n",
    "    tokenized_data = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)\n",
    "    return tokenized_data\n",
    "\n",
    "# Preprocess the training data (replace 'exp1_train.csv' with your actual file)\n",
    "train_dataset = preprocess_data(\"exp1_train.csv\", tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory for model checkpoints\n",
    "    # eval_strategy=\"epoch\",     # Evaluate after every epoch\n",
    "    learning_rate=5e-5,              # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size per device\n",
    "    per_device_eval_batch_size=16,   # Eval batch size\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    logging_dir=\"./logs\",            # Log directory\n",
    "    logging_steps=10,                # Log every 10 steps\n",
    "    save_total_limit=1,              # Limit number of saved checkpoints\n",
    "    predict_with_generate=True,      # Ensure generation during evaluation\n",
    ")\n",
    "\n",
    "# Data collator to pad batches to the max length\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,                         # The model you're training\n",
    "    args=training_args,                  # The training arguments\n",
    "    train_dataset=train_dataset,         # Your training dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer for input/output\n",
    "    data_collator=data_collator,         # Data collator for padding\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"models/enc_post_train\")\n",
    "tokenizer.save_pretrained(\"models/enc_post_train\")\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved to 'models/enc_post_train'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(\"eval_outputs\", exist_ok=True)\n",
    "\n",
    "def generate_corrections(model, tokenizer, inputs, max_len=128):\n",
    "    enc = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**enc, max_length=max_len)\n",
    "    return tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for eval_idx in range(1, 8):\n",
    "    for suffix in [\"ft_test\", \"train_test\"]:\n",
    "        dataset_name = f\"exp{eval_idx}_{suffix}.csv\"\n",
    "        df = pd.read_csv(dataset_name)\n",
    "        inputs = df[\"input\"].tolist()[:250]\n",
    "        targets = df[\"original_text\"].tolist()[:250]\n",
    "\n",
    "        # Evaluate with both train and ft models\n",
    "        for model_type in [\"train\", \"ft\"]:\n",
    "            for model_idx in range(1, 8):\n",
    "                model_name = f\"exp{model_idx}_{model_type}\"\n",
    "                model_dir = f\"models/{model_name}\"\n",
    "                if not os.path.exists(model_dir):\n",
    "                    continue  # Skip if model directory doesn't exist\n",
    "\n",
    "                print(f\"Evaluating model {model_name} on {dataset_name}...\")\n",
    "\n",
    "                model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(device).eval()\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "                preds = generate_corrections(model, tokenizer, inputs)\n",
    "\n",
    "                for inp, pred, tgt in zip(inputs, preds, targets):\n",
    "                    all_results.append({\n",
    "                        \"eval_dataset\": dataset_name,\n",
    "                        \"model\": model_name,\n",
    "                        \"perturbed_input\": inp,\n",
    "                        \"predicted_output\": pred,\n",
    "                        \"original_text\": tgt\n",
    "                    })\n",
    "\n",
    "# Save to one CSV\n",
    "out_df = pd.DataFrame(all_results)\n",
    "out_df.to_csv(\"eval_outputs/all_model_outputs.csv\", index=False)\n",
    "print(\"Saved all evaluation results (train + ft models) to eval_outputs/all_model_outputs.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"eval_outputs/all_model_outputs.csv\")\n",
    "\n",
    "df[\"predicted_output\"] = df[\"predicted_output\"].astype(str).str.strip()\n",
    "df[\"original_text\"] = df[\"original_text\"].astype(str).str.strip()\n",
    "\n",
    "df[\"exact_match\"] = df[\"predicted_output\"] == df[\"original_text\"]\n",
    "\n",
    "acc = df.groupby([\"model\", \"eval_dataset\"])[\"exact_match\"].mean().unstack()\n",
    "\n",
    "acc = (acc * 100).round(2)\n",
    "\n",
    "print(\"=== Exact Match Accuracy (%) ===\")\n",
    "print(acc.fillna(\"-\"))\n",
    "\n",
    "acc.to_csv(\"eval_outputs/accuracy_table.csv\")\n",
    "acc.to_markdown(\"eval_outputs/accuracy_table.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disspt2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
